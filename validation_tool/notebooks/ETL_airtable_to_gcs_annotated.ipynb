{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a27f5d62",
   "metadata": {},
   "source": [
    "Copy wholesale the `airtable_to_gcs.py` script here, then annotate it for personal use so we deconstruct what it is doing. We will make a similar one for the BlackCat API data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb072f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import os\n",
    "from typing import Optional\n",
    "\n",
    "import pandas as pd\n",
    "import pendulum\n",
    "from calitp_data_infra.auth import get_secret_by_name\n",
    "from calitp_data_infra.storage import get_fs, make_name_bq_safe\n",
    "from pyairtable import Table\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from airflow.models import BaseOperator\n",
    "\n",
    "\n",
    "def process_arrays_for_nulls(arr):\n",
    "    \"\"\"\n",
    "    BigQuery doesn't allow arrays that contain null values --\n",
    "    see: https://cloud.google.com/bigquery/docs/reference/standard-sql/data-types#array_nulls\n",
    "    Therefore we need to manually replace nulls with falsy values according\n",
    "    to the type of data in the array.\n",
    "    \"\"\"\n",
    "    types = set(type(entry) for entry in arr if entry is not None)\n",
    "\n",
    "    if not types:\n",
    "        return []\n",
    "    # use empty string for all non-numeric types\n",
    "    # may need to expand this over time\n",
    "    filler = -1 if types <= {int, float} else \"\"\n",
    "    return [x if x is not None else filler for x in arr]\n",
    "\n",
    "\n",
    "def make_arrays_bq_safe(raw_data):\n",
    "    safe_data = {}\n",
    "    for k, v in raw_data.items():\n",
    "        if isinstance(v, dict):\n",
    "            v = make_arrays_bq_safe(v)\n",
    "        elif isinstance(v, list):\n",
    "            v = process_arrays_for_nulls(v)\n",
    "        safe_data[k] = v\n",
    "    return safe_data\n",
    "\n",
    "\n",
    "# TODO: this should use the new generic partitioned GCS artifact type once available\n",
    "class AirtableExtract(BaseModel):\n",
    "    air_base_id: str\n",
    "    air_base_name: str\n",
    "    air_table_name: str\n",
    "    data: Optional[pd.DataFrame]\n",
    "    extract_time: Optional[pendulum.DateTime]\n",
    "\n",
    "    # pydantic doesn't know dataframe type\n",
    "    # see https://stackoverflow.com/a/69200069\n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "    def fetch_from_airtable(self, api_key):\n",
    "        \"\"\"Download an Airtable table as a DataFrame.\n",
    "\n",
    "        Note that airtable records have rows structured as follows:\n",
    "            [{\"id\", \"fields\": {colname: value, ...}, ...]\n",
    "\n",
    "        This function applies renames in the following order.\n",
    "\n",
    "            1. rename id\n",
    "            2. rename fields\n",
    "            3. apply column prefix (to columns not renamed by 1 or 2)\n",
    "        \"\"\"\n",
    "\n",
    "        print(\n",
    "            f\"Downloading airtable data for {self.air_base_name}.{self.air_table_name}\"\n",
    "        )\n",
    "        ## There's nly 1 table from this API; gathered here in a pyairtable Table object\n",
    "        all_rows = Table(api_key, self.air_base_id, self.air_table_name).all()\n",
    "\n",
    "        # this is not used until the `make_hive_path` function\n",
    "        self.extract_time = pendulum.now()\n",
    "\n",
    "        # convert the Table (list-like object?) to a DF while renaming, using the 1st 2 functions in this script\n",
    "        raw_df = pd.DataFrame(\n",
    "            [\n",
    "                {\"id\": row[\"id\"], **make_arrays_bq_safe(row[\"fields\"])}\n",
    "                for row in all_rows\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.data = raw_df.rename(make_name_bq_safe, axis=\"columns\")\n",
    "\n",
    "    ## Constructs the name of the GCS bucket & folder each table will be saved into\n",
    "    def make_hive_path(self, bucket: str):\n",
    "        if not self.extract_time:\n",
    "            # extract_time is usually set when airtable_to_df is called & data is retrieved\n",
    "            raise ValueError(\n",
    "                \"An extract time must be set before a hive path can be generated.\"\n",
    "            )\n",
    "        safe_air_table_name = (\n",
    "            str.lower(\"_\".join(self.air_table_name.split(\" \")))\n",
    "            .replace(\"-\", \"_\")\n",
    "            .replace(\"+\", \"and\")\n",
    "        )\n",
    "        \n",
    "        # These buckets and zipped files are visible in GCS\n",
    "        return os.path.join(\n",
    "            bucket,\n",
    "            f\"{self.air_base_name}__{safe_air_table_name}\",\n",
    "            f\"dt={self.extract_time.to_date_string()}\",\n",
    "            f\"ts={self.extract_time.to_iso8601_string()}\",\n",
    "            f\"{safe_air_table_name}.jsonl.gz\",\n",
    "        )\n",
    "\n",
    "    # Using function above, takes data from `fetch_from_airtable` and compresses it to JSON \n",
    "    def save_to_gcs(self, fs, bucket):\n",
    "        hive_path = self.make_hive_path(bucket)\n",
    "        print(f\"Uploading to GCS at {hive_path}\")\n",
    "        assert self.data.any(None), \"data does not exist, cannot save\"\n",
    "        fs.pipe(\n",
    "            hive_path,\n",
    "            gzip.compress(self.data.to_json(orient=\"records\", lines=True).encode()),\n",
    "        )\n",
    "        return hive_path\n",
    "\n",
    "\n",
    "### Uses all the classes and functions above it. Contains the `execute` function which brings all the functions together\n",
    "# This is the operator that is called in the Airflow DAGs, like here: \n",
    "# airflow/dags/airtable_loader_v2/california_transit_county_geography.yml\n",
    "class AirtableToGCSOperator(BaseOperator):\n",
    "    template_fields = (\"bucket\",)\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        bucket,\n",
    "        air_base_id,\n",
    "        air_base_name,\n",
    "        air_table_name,\n",
    "        api_key=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"An operator that downloads data from an Airtable base\n",
    "            and saves it as a JSON file hive-partitioned by date and time in Google Cloud\n",
    "            Storage (GCS).\n",
    "\n",
    "        Args:\n",
    "            bucket (str): GCS bucket where the scraped Airtable will be saved.\n",
    "            air_base_id (str): The underlying id of the Airtable instance being used.\n",
    "            air_base_name (str): The string name of the Base.\n",
    "            air_table_name (str): The table name that should be extracted from the\n",
    "                Airtable Base\n",
    "            api_key (str, optional): The API key to use when downloading from airtable.\n",
    "                This can be someone's personal API key. If not provided, the environment\n",
    "                variable of `CALITP_AIRTABLE_API_KEY` is used.\n",
    "        \"\"\"\n",
    "        self.bucket = bucket\n",
    "        \n",
    "        # calling the above class 'AirtableExtract'\n",
    "        self.extract = AirtableExtract(\n",
    "            air_base_id=air_base_id,\n",
    "            air_base_name=air_base_name,\n",
    "            air_table_name=air_table_name,\n",
    "        )\n",
    "        self.api_key = api_key\n",
    "\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def execute(self, **kwargs):\n",
    "        api_key = self.api_key or get_secret_by_name(\"CALITP_AIRTABLE_API_KEY\")\n",
    "        # calling the 'fetch_from_airtable' function within the class instance of 'AirtableExtract', \n",
    "        # which here is an object called `extract`.\n",
    "        self.extract.fetch_from_airtable(api_key)\n",
    "        fs = get_fs()\n",
    "        # inserts into xcoms\n",
    "        return self.extract.save_to_gcs(fs, self.bucket)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
